{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install chromadb\n",
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "OPENAI_API_BASE = os.environ['OPENAI_API_BASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('pdf_data/893.pdf')\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 split_docs\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# chunk_overlap=0 表示没有重叠的字符\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(pages)\n",
    "print(f'{len(split_docs)} split_docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "persist_directory = \"chroma_storage_893\"\n",
    "\n",
    "if os.path.exists(persist_directory) != True:\n",
    "    # 把分块内容处理成 embeddings\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        split_docs, embeddings, persist_directory=persist_directory)\n",
    "    # 结果持久化\n",
    "    vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: chroma_storage_893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='第 28 卷 / 第 4 期 / 2023  年4 月刘禹良，李鸿亮，白翔，金连文  \\n浅析 ChatGPT ： 历史沿革 、应用现状及前景展望\\nformer 的双向深层预训练模型 ，其模型参数首次超\\n过了 3亿规模 （BERT -Large 约有 3.4个参数 ） ；同年 ，\\nOpenAI 提出了生成式预训练  Transformer 模型 ——\\nGPT（generative pre -training ） （Radford 等，2018 ） ，大大\\n地推动了自然语言处理领域的发展 。2018 年，人工\\n智能系统 OpenAI Five （Berner 等，2019 ）战胜了世界\\n顶级的 Dota 2人类队伍 ，人工智能在复杂任务领域\\n树立了一个新的里程碑 ；此后 ，Google DeepMind 团\\n队提出的 AlphaFold （Jumper 等，2021 ）以前所未有的\\n准确度超越了人类研究者在蛋白质结构预测上的表\\n现，突破了人们对人工智能在生物学领域的应用的\\n想象 。2019 年，一种人工智能系统 AlphaStar （Viny ⁃\\nals等，2019 ）战胜了世界顶级的 StarCraft II 人类选\\n手，为人工智能在复杂任务领域的未来发展提供了\\n更加强有力的支持 。2020 年，随着 OpenAI GPT -3模\\n型（Brown 等，2020 ） （模型参数约 1 750亿）的问世 ，\\n在 众 多 自 然 语 言 处 理 （natural language processing ，\\nNLP）任务中 ，人工智能均表现出超过人类水平的能\\n力。2021 年1月，Google Brain 提出了 Switch Trans ⁃\\nformer 模型（Fedus 等，2021 ） ，以高达 1.6万亿的参数\\n量成为史上首个万亿级语言模型 ；同年 12月，谷歌\\n还提出了 1.2亿参数的通用稀疏模型 GLaM （Du等，\\n2022 ） ，在多个小样本学习任务的性能超过 GPT-3。\\n2022 年2月，人工智能生成内容 （artifical intelligence \\ngenerated content ，AIGC ）技 术 被 《MIT Technology \\nReview 》评选为 2022 年全球突破性技术之一 。同年', metadata={'source': 'pdf_data/893.pdf', 'page': 2}), Document(page_content='第 28 卷 / 第 4 期 / 2023  年4 月刘禹良，李鸿亮，白翔，金连文  \\n浅析 ChatGPT ： 历史沿革 、应用现状及前景展望\\n2\\u3000技术原理简介及可扩展性\\nChatGPT 是一种基于认知计算和人工智能的语\\n言 模 型 ，它 使 用 了  Transformer 架 构 （Vaswani 等，\\n2017 ）和GPT（Radford 等，2018 ） ，即生成式预训练技\\n术。GPT 训练的模型是一种应用于自然语言处理\\n（NLP）的模型 ，它通过使用多层 Transformer 来预测\\n下一个单词的概率分布 ，以生成自然语言文本 。这\\n是通过在超大型文本语料库上训练学习到的语言模\\n式来实现的 。\\n如表 2所示 ，从2018 年拥有 1.17亿参数的 GPT-1\\n（Radford 等，2018 ）到2020 年 拥 有 1 750亿 参 数 的\\nGPT-3（Brown 等，2020 ） ，OpenAI 的语言模型智能化\\n程度明显提升 。随着模型的不断增大 ，生成模型的\\n不断改进 ，以及自监督的不断完善 ，GPT 的语言处理\\n能力和生成能力得到了显著提升 。此后 ，2022 年\\n1月 基 于 RLHF （reinforcement learning from human \\nfeedback ）的InstructGPT （Ouyang 等，2022 ）的提出显\\n著降低了有害 、不真实和有偏差输出的概率 。在\\n2022 年12月，ChatGPT 作为 InstructGPT 的姐妹模型\\n被推出 。ChatGPT 在Instruct GPT 的基础上仅仅增\\n加了聊天属性 ，并且向公众开放了测试版本 。Chat⁃\\nGPT 的成功离不开多类技术的积累 ，除了生成式预\\n训练技术之外 ，其中最为核心的技术包括以下几个\\n方面 ：\\n1）RLHF 方法是一种基于人类偏好的强化学习\\n方法 。它通过利用人们对对话代理回答的评价来改\\n进对话代理的回答 。RLHF 方法可以根据人们的喜\\n好对对话代理的回答进行排序 ，例如通过考虑人们\\n喜欢的内容来选择文本摘要 。这些回答的评价用来\\n训练一个奖励模型 ，该模型将告诉强化学习系统如\\n何评价回答的好坏 。最后 ，通过强化学习训练对话代理来拟合这个奖励模型 。整个训练过程包括对\\nGPT-3进行监督微调 ，然后训练奖励模型 ，最后通过', metadata={'source': 'pdf_data/893.pdf', 'page': 4})]\n"
     ]
    }
   ],
   "source": [
    "query = '这篇论文和 Transformer 的相关性是什么？'\n",
    "# 加载保存在本地的文档数据\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embeddings)\n",
    "# 查询与提问最相关的2个文档\n",
    "search_docs = vectordb.similarity_search(query, 2)\n",
    "print(search_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 这篇论文和 Transformer 的相关性是什么？\n",
      "A: 这篇论文中提到了 ChatGPT 是一种基于认知计算和人工智能的语言模型，它使用了 Transformer 架构和 GPT，即生成式预训练技术。GPT 训练的模型是一种应用于自然语言处理（NLP）的模型，它通过使用多层 Transformer 来预测下一个单词的概率分布，以生成自然语言文本。因此，这篇论文和 Transformer 有很大的相关性，因为它们都是用于自然语言处理的模型，并且使用了 Transformer 架构。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "# 构建llm \n",
    "# temperature=0 意味着需要最保守的回答 减小模型产生幻觉的几率\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY,\n",
    "                 model_name='gpt-3.5-turbo', api_base=OPENAI_API_BASE)\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type='stuff')\n",
    "results = chain.run(input_documents=search_docs, question=query)\n",
    "print(f'Q: {query}')\n",
    "print(f'A: {results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<speak xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://www.w3.org/2001/mstts\" xmlns:emo=\"http://www.w3.org/2009/10/emotionml\" version=\"1.0\" xml:lang=\"en-US\"><voice name=\"zh-TW-YunJheNeural\"><prosody rate=\"20%\" pitch=\"20%\">这篇论文和 Transformer 的相关性是什么？这篇论文中提到了 ChatGPT 是一种基于认知计算和人工智能的语言模型，它使用了 Transformer 架构和 GPT，即生成式预训练技术。GPT 训练的模型是一种应用于自然语言处理（NLP）的模型，它通过使用多层 Transformer 来预测下一个单词的概率分布，以生成自然语言文本。因此，这篇论文和 Transformer 有很大的相关性，因为它们都是用于自然语言处理的模型，并且使用了 Transformer 架构。</prosody></voice></speak>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "speak = ET.Element('speak')\n",
    "speak.set('xmlns', 'http://www.w3.org/2001/10/synthesis')\n",
    "speak.set('xmlns:mstts', 'http://www.w3.org/2001/mstts')\n",
    "speak.set('xmlns:emo', 'http://www.w3.org/2009/10/emotionml')\n",
    "speak.set('version', '1.0')\n",
    "speak.set('xml:lang', 'en-US')\n",
    "\n",
    "voice = ET.SubElement(speak, 'voice')\n",
    "voice.set('name', 'zh-TW-YunJheNeural')\n",
    "\n",
    "prosody = ET.SubElement(voice, 'prosody')\n",
    "prosody.set('rate', '20%')\n",
    "prosody.set('pitch', '20%')\n",
    "prosody.text = query + results\n",
    "\n",
    "ET.dump(speak)\n",
    "tree = ET.ElementTree(speak)\n",
    "tree.write(persist_directory + '/SSML.xml', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
